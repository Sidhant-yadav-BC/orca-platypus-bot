{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T08:42:47.741137Z","iopub.execute_input":"2023-08-18T08:42:47.741504Z","iopub.status.idle":"2023-08-18T08:42:47.764391Z","shell.execute_reply.started":"2023-08-18T08:42:47.741474Z","shell.execute_reply":"2023-08-18T08:42:47.763252Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q langchain torch accelerate bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:04:13.508868Z","iopub.execute_input":"2023-08-21T09:04:13.509321Z","iopub.status.idle":"2023-08-21T09:04:27.068920Z","shell.execute_reply.started":"2023-08-21T09:04:13.509281Z","shell.execute_reply":"2023-08-21T09:04:27.067326Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:04:27.071888Z","iopub.execute_input":"2023-08-21T09:04:27.072302Z","iopub.status.idle":"2023-08-21T09:04:27.080591Z","shell.execute_reply.started":"2023-08-21T09:04:27.072261Z","shell.execute_reply":"2023-08-21T09:04:27.078611Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# To load model \nmodel_id = \"Open-Orca/OpenOrca-Platypus2-13B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel_8 = AutoModelForCausalLM.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\", device_map = \"auto\", load_in_8bit= True)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:25:56.993195Z","iopub.execute_input":"2023-08-21T09:25:56.993640Z","iopub.status.idle":"2023-08-21T09:28:33.741026Z","shell.execute_reply.started":"2023-08-21T09:25:56.993584Z","shell.execute_reply":"2023-08-21T09:28:33.737664Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673413855520435a83289a29b2d5e230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce21ce50ce3541f0b6fbaca11a84f53e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f121c650d9dc49af8e51d25cb00895e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e0af08a7bd46ebbcfd43755927b829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3741af6b757f4935b9f767575f81b8ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495892a576ce49bd823abc9034b35e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09720b21334f4bc5b318e9e25232c1ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6184110c1e438eadd62f44ceee5b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2eb32c1873c46c9bb75e65742bb392a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231d5598c0d249e6b06117db81d44b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3588ae2a7a4c11b5a4d27da7a7e2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mmodel_id = \u001b[33m\"\u001b[0m\u001b[33mOpen-Orca/OpenOrca-Platypus2-13B\u001b[0m\u001b[33m\"\u001b[0m                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mtokenizer = AutoTokenizer.from_pretrained(model_id)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5 model_8 = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[0m\u001b[33mOpen-Orca/OpenOrca-Platypus2-13B\u001b[0m\u001b[33m\"\u001b[0m, devic     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m484\u001b[0m in          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mtype\u001b[0m(config) \u001b[95min\u001b[0m \u001b[96mcls\u001b[0m._model_mapping.keys():                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m483 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m484 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m487 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2675\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2672 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_contexts.append(init_empty_weights())                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2673 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2674 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ContextManagers(init_contexts):                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2675 \u001b[2m│   │   │   \u001b[0mmodel = \u001b[96mcls\u001b[0m(config, *model_args, **model_kwargs)                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2676 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2677 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Check first if we are `from_pt`\u001b[0m                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2678 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m use_keep_in_fp32_modules:                                                      \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mLlamaForCausalLM.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'max_new_tokens'\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>model_id = <span style=\"color: #808000; text-decoration-color: #808000\">\"Open-Orca/OpenOrca-Platypus2-13B\"</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>tokenizer = AutoTokenizer.from_pretrained(model_id)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5 model_8 = AutoModelForCausalLM.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">\"Open-Orca/OpenOrca-Platypus2-13B\"</span>, devic     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">484</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">482 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(config) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping.keys():                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_class = _get_model_class(config, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>484 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2675</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2672 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>init_contexts.append(init_empty_weights())                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2673 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2674 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ContextManagers(init_contexts):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2675 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(config, *model_args, **model_kwargs)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2676 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2677 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Check first if we are `from_pt`</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2678 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_keep_in_fp32_modules:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaForCausalLM.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'max_new_tokens'</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model_8bit = model","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:12:23.460449Z","iopub.execute_input":"2023-08-21T06:12:23.460854Z","iopub.status.idle":"2023-08-21T06:12:23.465987Z","shell.execute_reply.started":"2023-08-21T06:12:23.460821Z","shell.execute_reply":"2023-08-21T06:12:23.464612Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info= '\\n'.join(gpu_info)\nprint(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T05:00:30.869471Z","iopub.execute_input":"2023-08-21T05:00:30.869828Z","iopub.status.idle":"2023-08-21T05:00:30.979202Z","shell.execute_reply.started":"2023-08-21T05:00:30.869795Z","shell.execute_reply":"2023-08-21T05:00:30.978053Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mon Aug 21 05:00:30 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0    29W /  70W |   7778MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   56C    P0    28W /  70W |   7778MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# to generate text from loaded model\nname = \"Open-Orca/OpenOrca-Platypus2-13B\"\ntext = \"what is the capital of india?\"\nmax_new_tokens = 1000\n\ndef generate_from_model(model, tokenizer, text):\n  encoded_input = tokenizer(text, return_tensors='pt')\n  output_sequences = model.generate(input_ids=encoded_input['input_ids'].cuda())\n  return tokenizer.decode(output_sequences[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:11:47.104676Z","iopub.execute_input":"2023-08-21T06:11:47.105072Z","iopub.status.idle":"2023-08-21T06:11:47.113841Z","shell.execute_reply.started":"2023-08-21T06:11:47.105035Z","shell.execute_reply":"2023-08-21T06:11:47.112857Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"print(generate_from_model(model, tokenizer, text))","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:11:48.329962Z","iopub.execute_input":"2023-08-21T06:11:48.330379Z","iopub.status.idle":"2023-08-21T06:11:52.390203Z","shell.execute_reply.started":"2023-08-21T06:11:48.330347Z","shell.execute_reply":"2023-08-21T06:11:52.389176Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"what is the capital of india?\n\nThe capital of India is New Delhi.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n\npipe = pipeline(\n    \"text-generation\",\n    model=model, \n    tokenizer=tokenizer, \n    max_length=500\n)\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:21:52.616953Z","iopub.execute_input":"2023-08-21T06:21:52.617494Z","iopub.status.idle":"2023-08-21T06:21:52.626583Z","shell.execute_reply.started":"2023-08-21T06:21:52.617450Z","shell.execute_reply":"2023-08-21T06:21:52.625164Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n\ntemplate = \"\"\"you are an expert in python languge who returns only the python code for following user query: \nquery: {question}\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:17:17.977838Z","iopub.execute_input":"2023-08-21T06:17:17.978575Z","iopub.status.idle":"2023-08-21T06:17:17.985339Z","shell.execute_reply.started":"2023-08-21T06:17:17.978528Z","shell.execute_reply":"2023-08-21T06:17:17.984223Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"llm_chain = LLMChain(prompt=prompt, \n                     llm=local_llm\n                     )\n\nquestion = \"print a stacked bar chart\"\nprint(llm_chain.run(question))","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:21:54.500250Z","iopub.execute_input":"2023-08-21T06:21:54.501161Z","iopub.status.idle":"2023-08-21T06:24:58.963201Z","shell.execute_reply.started":"2023-08-21T06:21:54.501121Z","shell.execute_reply":"2023-08-21T06:24:58.962093Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":" with the given data\ndata: [\n    ['Apple', 10],\n    ['Orange', 20],\n    ['Banana', 30],\n    ['Mango', 40],\n    ['Pineapple', 50]\n]\n\nHere's the Python code to create a stacked bar chart:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\ndata = [\n    ['Apple', 10],\n    ['Orange', 20],\n    ['Banana', 30],\n    ['Mango', 40],\n    ['Pineapple', 50]\n]\n\ncolors = ['#F7B6D2', '#FDB45C', '#729FDB', '#F781BE', '#008080']\n\nrects = []\nfor name, count in data:\n    rects.append(plt.Rectangle((0, 0), 1, count, name))\n\nfor rect in rects:\n    ax.add_patch(rect)\n\nplt.show()\n\nIn this code, we create a stacked bar chart using matplotlib.pyplot library. The data is provided in the form of a list of lists, with each inner list containing the name of the fruit and its count. The colors are assigned to each bar using the colors list. Finally, the chart is displayed using plt.show().\n\nPlease note that this code only provides the Python implementation for creating a stacked bar chart. To actually print the chart, you would need to use a command-line interface or integrate this code into a GUI application.\n\nIf you need further assistance or customization, please provide more details about your specific requirements.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:12:34.324800Z","iopub.execute_input":"2023-08-21T09:12:34.325213Z","iopub.status.idle":"2023-08-21T09:12:34.333628Z","shell.execute_reply.started":"2023-08-21T09:12:34.325177Z","shell.execute_reply":"2023-08-21T09:12:34.332674Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<replicate.client.Client at 0x7a2f91437250>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}